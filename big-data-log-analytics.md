---
subcollection: solution-tutorials
copyright:
  years: 2018, 2019
lastupdated: "2019-10-31"
lasttested: "2019-10-31"
---

{:shortdesc: .shortdesc}
{:new_window: target="_blank"}
{:codeblock: .codeblock}
{:screen: .screen}
{:tip: .tip}
{:pre: .pre}

# Big data logs with streaming analytics and SQL
{: #big-data-log-analytics}

In this tutorial, you will build a log analysis pipeline designed to collect, store and analyze log records to support regulatory requirements or aid information discovery. This solution leverages several services available in {{site.data.keyword.cloud_notm}}: {{site.data.keyword.messagehub}}, {{site.data.keyword.cos_short}}, {{site.data.keyword.sqlquery_short}}, {{site.data.keyword.streaminganalyticsshort}} and {{site.data.keyword.iae_full_notm}}. A program will assist you by simulating transmission of web server log messages from a static file to {{site.data.keyword.messagehub}}.

With {{site.data.keyword.messagehub}} the pipeline scales to receive millions of log records from a variety of producers. Using a combination of {{site.data.keyword.streaminganalyticsshort}} and {{site.data.keyword.sqlquery_short}}, or {{site.data.keyword.iae_full_notm}}, log data can be inspected in realtime to integrate business processes. Log messages can also be easily redirected to long term storage using {{site.data.keyword.cos_short}} where developers, support staff and auditors can work directly with data.

While this tutorial focuses on log analysis, it is applicable to other scenarios: storage-limited IoT devices can similarly stream messages to {{site.data.keyword.cos_short}} or marketing professionals can segment and analyze customer events across digital properties with SQL Query.
{:shortdesc}

## Objectives
{: #objectives}

* Understand Apache Kafka publish-subscribe messaging
* Store log data for audit and compliance requirements
* Monitor logs to create exception handling processes
* Conduct forensic and statistical analysis on log data

## Services used
{: #services}

This tutorial uses the following runtimes and services:

* [{{site.data.keyword.cos_short}}](https://{DomainName}/catalog/services/cloud-object-storage)
* [{{site.data.keyword.messagehub}}](https://{DomainName}/catalog/services/message-hub)
* [{{site.data.keyword.sqlquery_short}}](https://{DomainName}/catalog/services/sql-query)
* [{{site.data.keyword.streaminganalyticsshort}}](https://{DomainName}/catalog/services/streaming-analytics)
* [{{site.data.keyword.iae_short}}](https://{DomainName}/catalog/services/analytics-engine)

This tutorial may incur costs. Use the [Pricing Calculator](https://{DomainName}/estimator/review) to generate a cost estimate based on your projected usage.

## Architecture
{: #architecture}

<p style="text-align: center;">

  ![Architecture](images/solution31/Architecture.png)
</p>

1. Application generates log events to {{site.data.keyword.messagehub}}.
2. Log event is intercepted and analyzed by {{site.data.keyword.streaminganalyticsshort}}.
3. Log event is appended to a CSV file located in {{site.data.keyword.cos_short}}.
4. Auditor or support staff use {{site.data.keyword.sqlquery_short}} or {{site.data.keyword.iae_short}} to perform requests.
5. Requests are executed against the data stored in {{site.data.keyword.cos_short}}.

## Before you begin
{: #prereqs}

* [Install Git](https://git-scm.com/)
* [Install {{site.data.keyword.Bluemix_notm}} CLI](https://{DomainName}/docs/cli?topic=cloud-cli-getting-started)
* [Install Node.js](https://nodejs.org)

## Create services
{: #setup}

In this section, you will create the services required to perform analysis of log events generated by your applications.

### {{site.data.keyword.cos_short}}
{: #new-cos}

1. Create an instance of [{{site.data.keyword.cos_short}}](https://{DomainName}/catalog/services/cloud-object-storage).
   1. Set **Service name** to **log-analysis-cos**.
   1. Select the **Lite** plan or the **Standard** plan if you already have an {{site.data.keyword.cos_short}} service instance in your account.
   1. Click **Create**
1. Under **Service Credentials**, create new credential and select **Include HMAC Credential**.
   1. Make note of the _access_key_id_ and _secret_access_key_ values.
1. Create a bucket named `<your-initial>-log-analysis` with **Cross Region** resiliency.
1. Under **Endpoint**, find the **private** endpoint to access your bucket.

### {{site.data.keyword.messagehub}}
{: #new-eventstreams}

1. Create an instance of [{{site.data.keyword.messagehub}}](https://{DomainName}/catalog/services/event-streams).
   1. Set the **Service name** to **log-analysis-hub**.
   1. Select the **Lite** plan.
   1. Click **Create**.
1. Switch to **Topics**, click **Create topic**
   1. Set the **Topic Name** to `webserver`.
   1. Click the **Create topic** button.
1. Under **Service credentials**, create new credential named `webserver-flow`.
1. Make note of the values. They will be used in the next section.

### {{site.data.keyword.sqlquery_short}}
{: #new-sqlquery}

1. Create an instance of [{{site.data.keyword.sqlquery_short}}](https://{DomainName}/catalog/services/sql-query).
   1. Set the **Service name** to **log-analysis-sql**.
   1. Select the **Lite** plan.
   1. Click **Create**.

### {{site.data.keyword.iae_short}}
{: #new-iae}

1. Create an instance of [{{site.data.keyword.iae_short}}](https://{DomainName}/catalog/services/analytics-engine).
   1. Set the **Service name** to **log-analysis-iae**
   1. Click on Configure. 
1. Set **Hardware configuration** to **Default**.
1. Set **Number of compute nodes** to **1**.
1. Select the **latest** version of **Spark and Hadoop** as the **Software Package**.
1. Under Advanced Options enter the below given configuration options for the {{site.data.keyword.cos_short}} which was created in the previous step. 

   ```json
   {
     "core-site": {
       "fs.cos.<identifier>.access.key": "<access_key_id>",
       "fs.cos.<identifier>.secret.key": "<secret_access_key>",
       "fs.cos.<identifier>.endpoint": "<cosEndpoint>"
     }
   }
   ```
   {: codeblock}

   where:
      - `identifier` is the name of the name of the {{site.data.keyword.cos_short}} service (`log-analysis-cos`),
      - `access_key_id` and `secret_access_key` are found in the service credentials created earlier.
      - `cosEndpoint` is a private endpoint to access the {{site.data.keyword.cos_short}} bucket.
1. Once the service is provisioned, go to **Manage** to retrieve the user name and password for the cluster. You may need to reset the cluster password.
1. Under **Service credentials**, create new credential.
1. From the credentials, make note of the `ssh` value giving the *ssh* command line to execute to connect to the cluster.

## Process log messages with Streams in Watson Data Platform
{: #configure-streams}

### Create a Streams flow source
{: #streamsflow}

In this section, you will begin configuring a Streams flow that receives log messages. The {{site.data.keyword.streaminganalyticsshort}} service is powered by {{site.data.keyword.streamsshort}}, which can analyze millions of events per second, enabling sub-millisecond response times and instant decision-making.

1. In your browser, access [Watson Data Platform](https://dataplatform.ibm.com).
2. Select the **New project** button or tile, then the **Basic** tile and click **OK**.
    * Enter the **Name** `webserver-logs`.
    * The **Storage** option should be set to `log-analysis-cos`. If not, select the service instance.
    * Click the **Create** button.
4. Click the **Add to project** button then **Streams flow** from the top navigation bar.
    * Click **Associate an IBM Streaming Analytics instance with a container-based plan**.
    * Create a new {{site.data.keyword.streaminganalyticsshort}} instance by selecting the **Lite** radio button and clicking **Create**.
    * Provide the **Service name** as `log-analysis-sa` and click **Confirm**.
    * Type the streams flow **Name** as `webserver-flow`.
    * Finish by clicking **Create**.
5. On the resulting page, select the **{{site.data.keyword.messagehub}}** tile.
    * Click **Add Connection** and select your `log-analysis-hub` {{site.data.keyword.messagehub}} instance. If you do not see your instance listed, select the **IBM {{site.data.keyword.messagehub}}** option. Manually enter the **Connection details** that you obtained from the **Service credentials** in the previous section. **Name** the connection `webserver-flow`.
    * Click **Create** to create the connection.
    * Select `webserver` from the **Topic** dropdown.
    * Select **Start with the first new message** from the **Initial Offset** dropdown.
    * Click **Continue**.
6. Leave the **Preview Data** page open; it will be used in the next section.

### Using Kafka console tools with {{site.data.keyword.messagehub}}
{: #kafkatools}

The `webserver-flow` is currently idle and awaiting messages. In this section, you will configure Kafka console tools to work with {{site.data.keyword.messagehub}}. Kafka console tools allow you to produce arbitrary messages from the terminal and send them to {{site.data.keyword.messagehub}}, which will trigger the `webserver-flow`.

1. Download and unzip the [Kafka 0.10.2.X client](https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz).
2. Change directory to `bin` and create a text file named `message-hub.config` with the following contents.
    ```sh
    sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="USER" password="PASSWORD";
    security.protocol=SASL_SSL
    sasl.mechanism=PLAIN
    ssl.protocol=TLSv1.2
    ssl.enabled.protocols=TLSv1.2
    ssl.endpoint.identification.algorithm=HTTPS
    ```
    {: codeblock}
3. Replace `USER` and `PASSWORD` in your `message-hub.config` file with the `user` and `password` values seen in **Service Credentials** from the previous section. Save `message-hub.config`.
4. From the `bin` directory, run the following command. Replace `KAFKA_BROKERS_SASL` with the `kafka_brokers_sasl` value seen in **Service Credentials**. An example is provided.
    ```sh
    ./kafka-console-producer.sh --broker-list KAFKA_BROKERS_SASL --producer.config message-hub.config --topic webserver
    ```
    {: pre}
    ```sh
    ./kafka-console-producer.sh --broker-list \
    kafka04-prod02.messagehub.services.us-south.bluemix.net:9093,\
    kafka05-prod02.messagehub.services.us-south.bluemix.net:9093,\
    kafka02-prod02.messagehub.services.us-south.bluemix.net:9093,\
    kafka01-prod02.messagehub.services.us-south.bluemix.net:9093,\
    kafka03-prod02.messagehub.services.us-south.bluemix.net:9093 \
    --producer.config message-hub.config --topic webserver
    ```
5. The Kafka console tool is awaiting input. Copy and paste the log message from below into the terminal. Hit `enter` to send the log message to {{site.data.keyword.messagehub}}. Notice the sent messages also display on the `webserver-flow` **Preview Data** page.
    ```javascript
    { "host": "199.72.81.55", "timestamp": "01/Jul/1995:00:00:01 -0400", "request": "GET /history/apollo/ HTTP/1.0", "responseCode": 200, "bytes": 6245 }
    ```
    {: codeblock}
![Preview page](images/solution31/preview_data.png)

### Create a Streams flow target
{: #streamstarget}

In this section, you will complete the streams flow configuration by defining a target. The target will be used to store incoming log messages in the {{site.data.keyword.cos_short}} bucket created earlier. The process of storing and appending incoming log messages to a file will be done automatically by {{site.data.keyword.streaminganalyticsshort}}.

1. On the `webserver-flow` **Preview Page**, click the **Continue** button.
2. Select the **{{site.data.keyword.cos_full_notm}}** tile as a target.
    * Click **Add Connection** and select `log-analysis-cos`.
    * Click **Create**.
    * Enter the **File path** `/YOUR_BUCKET_NAME/logs/http-logs_%TIME.csv`. Replace `YOUR_BUCKET_NAME` with the one created in the first section.
    * Select **csv** in the **Format** dropdown.
    * Check the **Column header row** checkbox.
    * Select **Time** in the **File Creation Policy** dropdown.
    * Set the **Amount of time** to **60** seconds.
    * Click **Continue**.
3. Click **Save**.
4. Click the **>** play button to **Start the streams flow**.
5. After the flow is started, again send multiple log messages from the Kafka console tool. You can watch as messages arrive by viewing the `webserver-flow` in Streams Designer.
    ```javascript
    { "host": "199.72.81.55", "timestamp": "01/Jul/1995:00:00:01 -0400", "request": "GET /history/apollo/ HTTP/1.0", "responseCode": 200, "bytes": 6245 }
    ```
    {: codeblock}
6. Return to your bucket in {{site.data.keyword.cos_short}}. New CSV files will added 60 seconds after messages have entered the flow or the flow is restarted.
   ![webserver-flow](images/solution31/flow.png)

### Add conditional behavior to Streams flows
{: #streamslogic}

Up to now, the Streams flow is a simple pipe - moving messages from {{site.data.keyword.messagehub}} to {{site.data.keyword.cos_short}}. More than likely, teams will want to know events of interest in realtime. For example individual teams might benefit from alerts when HTTP 500 (application error) events occur. In this section, you will add conditional logic to the flow to identify HTTP 200 (OK) and non HTTP 200 codes.

1. Use the pencil button to **Edit the streams flow**.
2. Create a filter node that handles HTTP 200 responses.
   * From the **Nodes** palette, drag the **Filter** node from **PROCESSING AND ANALYTICS** to the canvas.
   * Type `OK` in the name textbox, which currently contains the word `Filter`.
   * Enter the following statement in the **Condition Expression** text area.
      ```sh
      responseCode == 200
      ```
      {: codeblock}
   * With your mouse, draw a line from the **{{site.data.keyword.messagehub}}** node's output (right side) to your **OK** node's input (left side).
   * From the **Nodes** palette, drag the **Debug** node found under **TARGETS** to the canvas.
   * Connect the **Debug** node to the **OK** node by drawing a line between the two.
3. Repeat the process to create a `Not OK` filter using the same nodes and the following condition statement.
   ```sh
   responseCode >= 300
   ```
   {: codeblock}
4. Click the play button to **Save and run the streams flow**.
5. When prompted click the link to **run the new version**.
   ![Flow designer](images/solution31/flow_design.png)

### Increasing message load
{: #streamsload}

To view conditional handling in your Streams flow, you will increase the message volume sent to {{site.data.keyword.messagehub}}. The provided Node.js program simulates a realistic flow of messages to {{site.data.keyword.messagehub}} based on traffic to the webserver. To demonstrate the scalability of {{site.data.keyword.messagehub}} and {{site.data.keyword.streaminganalyticsshort}}, you will increase the throughput of log messages.

This section uses [node-rdkafka](https://www.npmjs.com/package/node-rdkafka). See the npmjs page for troubleshooting instructions if the simulator installation fails. If problems persist, you can skip to the next section and manually upload the data.

1. Download and unzip the [Jul 01 to Jul 31, ASCII format, 20.7 MB gzip compressed](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz) log file from NASA.
2. Clone and install the log simulator from [IBM-Cloud on GitHub](https://github.com/IBM-Cloud/kafka-log-simulator).
    ```sh
    git clone https://github.com/IBM-Cloud/kafka-log-simulator.git
    ```
    {: pre}
3. Change to the simulator's directory and run the following commands to setup the simulator and produce log event messages. Replace `LOGFILE` with the file you downloaded. Replace `BROKERLIST` and `APIKEY` with the corresponding **Service Credentials** used earlier. An example is provided.
    ```sh
    npm install
    ```
    {: pre}
    ```sh
    npm run build
    ```
    {: pre}
    ```sh
    node dist/index.js --file <LOGFILE> --parser httpd --broker-list <BROKERLIST> \
    --api-key <APIKEY> --topic webserver --rate 100
    ```
    {: pre}
    ```sh
    node dist/index.js --file /Users/ibmcloud/Downloads/NASA_access_log_Jul95 \
    --parser httpd --broker-list \
    "kafka04-prod02.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,\
    kafka05-prod02.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,\
    kafka02-prod02.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,\
    kafka01-prod02.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,\
    kafka03-prod02.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093" \
    --api-key 12345678901234567890 \
    --topic webserver --rate 100
    ```
4. In your browser, return to your `webserver-flow` after the simulator begins producing messages.
5. Stop the simulator after a desired number of messages have gone through the conditional branches using `control+C`.
6. Experiment with {{site.data.keyword.messagehub}} scaling by increasing or decreasing the `--rate` value.
   ![Flow load set to 10](images/solution31/flow_load_10.png)

The simulator will delay sending the next message based on the elapsed time in the webserver log. Setting `--rate 1` sends events in realtime. Setting `--rate 100` means that for every 1 second of elapsed time in the webserver log a 10ms delay between messages is used. Setting `--rate 0` sends all events immediately with no delay between events.
{: tip}


## Investigating log data using {{site.data.keyword.sqlquery_short}}
{: #sqlquery}

Depending on how long you ran the simulator, the number of files on {{site.data.keyword.cos_short}} has certainly grown. You will now act as an investigator answering audit or compliance questions by combining {{site.data.keyword.sqlquery_short}} with your log file. The benefit of using {{site.data.keyword.sqlquery_short}} is that the log file is directly accessible - no additional transformations or database servers are necessary.

If you prefer not to wait for the simulator to send all log messages, upload a [sample CSV file](https://github.com/IBM-Cloud/kafka-log-simulator/blob/master/data/http-logs_20191031_143106.csv.gz) to {{site.data.keyword.cos_short}} to get started immediately. You can use the {{site.data.keyword.cos_short}} plugin for the {{site.data.keyword.cloud_notm}} CLI to upload the file to the bucket: `ibmcloud cos upload --bucket <YOUR_BUCKET_NAME> --key logs/http-logs_20191031_143106.csv.gz  --file http-logs_20191031_143106.csv.gz --region us-geo`.
{: tip}

1. Access the `log-analysis-sql` service instance from the [Resource List](https://{DomainName}/resources?search=log-analysis). Select **Launch {{site.data.keyword.sqlquery_short}} UI** to launch {{site.data.keyword.sqlquery_short}}.
2. Enter the following SQL into the **Type SQL here ...** text area.
    ```sql
    -- What are the top 10 web pages on NASA from July 1995?
    -- Which mission might be significant?
    SELECT REQUEST, COUNT(REQUEST)
    FROM cos://us-geo/YOUR_BUCKET_NAME/logs/http-logs_TIME.csv
    WHERE REQUEST LIKE '%.htm%'
    GROUP BY REQUEST
    ORDER BY 2 DESC
    LIMIT 10
    ```
    {: codeblock}
3. Retrieve the Object SQL URL from the logs file.
    * From the [Resource List](https://{DomainName}/resources?search=log-analysis), select the `log-analysis-cos` service instance.
    * Select the bucket you created previously.
    * Click the overflow menu on the `http-logs_TIME.csv` file and select **Object SQL URL**.
    * **Copy** the URL to the clipboard.
4. Update the `FROM` clause with your Object SQL URL and click **Run**.
5. The result can be seen on the **Result** tab. While some pages - like the Kennedy Space Center home page - are expected one mission is quite popular at the time.
6. Select the **Query Details** tab to view additional information such as the location where the result was stored on {{site.data.keyword.cos_short}}.
7. Try the following question and answer pairs by adding them individually to the **Type SQL here ...** text area.
    ```sql
    -- Who are the top 5 viewers?
    SELECT HOST, COUNT(*)
    FROM cos://us-geo/YOUR_BUCKET_NAME/logs/http-logs_TIME.csv
    GROUP BY HOST
    ORDER BY 2 DESC
    LIMIT 5
    ```
    {: codeblock}

    ```sql
    -- Which viewer has suspicious activity based on application failures?
    SELECT HOST, COUNT(*)
    FROM cos://us-geo/YOUR_BUCKET_NAME/logs/http-logs_TIME.csv
    WHERE `responseCode` == 500
    GROUP BY HOST
    ORDER BY 2 DESC
    ```
    {: codeblock}

    ```sql
    -- Which requests showed a page not found error to the user?
    SELECT DISTINCT REQUEST
    FROM cos://us-geo/YOUR_BUCKET_NAME/logs/http-logs_TIME.csv
    WHERE `responseCode` == 404
    ```
    {: codeblock}

    ```sql
    -- What are the top 10 largest files?
    SELECT DISTINCT REQUEST, BYTES
    FROM cos://us-geo/YOUR_BUCKET_NAME/logs/http-logs_TIME.csv
    WHERE BYTES > 0
    ORDER BY CAST(BYTES as Integer) DESC
    LIMIT 10
    ```
    {: codeblock}

    ```sql
    -- What is the distribution of total traffic by hour?
    SELECT SUBSTRING(TIMESTAMP, 13, 2), COUNT(*)
    FROM cos://us-geo/YOUR_BUCKET_NAME/logs/http-logs_TIME.csv
    GROUP BY 1
    ORDER BY 1 ASC
    ```
    {: codeblock}

    ```sql
    -- Why did the previous result return an empty hour?
    -- Hint, find the malformed hostname.
    SELECT HOST, REQUEST
    FROM cos://us-geo/YOUR_BUCKET_NAME/logs/http-logs_TIME.csv
    WHERE SUBSTRING(TIMESTAMP, 13, 2) == ''
    ```
    {: codeblock}

FROM clauses are not limited to a single file. Use `cos://us-geo/YOUR_BUCKET_NAME/logs/` to run SQL queries on all files in the bucket.
{: tip}

## Investigating data using {{site.data.keyword.iae_short}}

### Investigating log data using Apache Hive
{: #hive}

Just as you ran queries using {{site.data.keyword.sqlquery_short}}, you can also run SQL analytical commands from Apache Hive that is part of the {{site.data.keyword.iae_short}} service.

1. First SSH to the {{site.data.keyword.iae_short}} cluster using the following command
   ```sh
   ssh clsadmin@chs-xxxxx-mn003.<changeme>.ae.appdomain.cloud 
   ```
   {: pre}
2. Connect to the Hive server by using with Beeline client.
   ```sh
   beeline -u ‘jdbc:hive2://chs-xxxxx-mn001.<change-me>.ae.appdomain.cloud:8443/;ssl=true;transportMode=http;httpPath=gateway/default/hive’ -n clsadmin -p <password>
   ```
   {: pre}
   The hive_jdbc service endpoint can be found under the service credential tab of the IAE resource page. 
3. Create an external hive table with the following command.
   ```sql
   CREATE EXTERNAL TABLE myhivetable (event_key string, event_topic string, event_offset int, event_partition int,event_timestamp string, host string, ts string, request string, responseCode int,bytes int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 'cos://<YOUR_BUCKET_NAME>.<identifer>/logs/' tblproperties ("skip.header.line.count"="1");
   ```
   {: codeblock}

   The value of `<identifer>` will be the same one that you defined during the creation of {{site.data.keyword.iae_short}} initially. Note that for Hive you need to point to a parent folder which contains the CSV file. In the example above, the data got written in the `logs` folder for this purpose.
4. Just like the commands executed earlier SQL queries can be executed on the table. For example:
   ```sql
   SELECT HOST, COUNT(*)
   FROM myhivetable
   GROUP BY HOST
   ORDER BY 2 DESC
   LIMIT 5;
   ```
   {: codeblock}

### Investigating data using Spark SQL
{: #sparksql}

The data pushed to cos can be also queried using Apache Spark that is part of the {{site.data.keyword.iae_short}} service.

1. SSH to the cluster
   ```sh
   ssh clsadmin@chs-xxxxx-mn003.<changeme>.ae.appdomain.cloud 
   ```
   {: pre}
2. Open a pyspark-shell on your {{site.data.keyword.iae_short}} cluster. 
   ```sh
   pyspark
   ```
   {: pre}
3. Create a Spark dataframe of a csv file which is present in the {{site.data.keyword.cos_short}} bucket. Note that the {{site.data.keyword.cos_short}} credentials have already been added to the {{site.data.keyword.iae_short}} cluster during set up.
   ```sh
   df = spark.read.csv('cos://<bucketname>.<identifer>/<objectname>')
   ```
   {: codeblock}
   For example if the name of the bucket is `john-log-analysis`, service name is `log-analysis-cos` and the path to the file is nasadata/: 
   ```sh
   df = spark.read.csv('cos://john-log-analysis.log-analysis-cos/nasadata/NASA_access_log_Jul95.csv')
   ```
   {: codeblock}
4. Any SQL query can be performed on the data and the result can be stored in a new dataframe.
5. The following code block will perform an SQL query the data frame. A view is then created and first 10 rows are printed.
   ```sh
   sqlContext.registerDataFrameAsTable(df, "Table")
   df_query = sqlContext.sql("SELECT * FROM Table LIMIT 10")
   df_query.show(10)
   ```
   {: codeblock}
   The query given here can be replaced with any other query which needs to be performed. 

## Expand the tutorial
{: #expand}

Congratulations, you have built a log analysis pipeline with {{site.data.keyword.cloud_notm}}. Below are additional suggestions to enhance your solution.

* Use additional targets in Streams Designer to store data in [{{site.data.keyword.cloudant_short_notm}}](https://{DomainName}/catalog/services/cloudant) or execute code in [{{site.data.keyword.openwhisk_short}}](https://{DomainName}/openwhisk)
* Follow the [Build a data lake using Object Storage](https://{DomainName}/docs/tutorials?topic=solution-tutorials-smart-data-lake#build-a-data-lake-using-object-storage) tutorial to add a dashboard to log data
* Integrate additional systems with {{site.data.keyword.messagehub}} using [{{site.data.keyword.appconserviceshort}}](https://{DomainName}/catalog/services/app-connect).

## Remove services
{: #removal}

From the [Resource List](https://{DomainName}/resources?search=log-analysis), use the **Delete** or **Delete service** menu item in the overflow menu to remove the following service instances.

* log-analysis-sa
* log-analysis-hub
* log-analysis-sql
* log-analysis-cos

## Related content
{:related}

* [Apache Kafka](https://kafka.apache.org/)
* [Configure a {{site.data.keyword.cos_full_notm}} connection through Ambari](https://{DomainName}/docs/services/AnalyticsEngine?topic=AnalyticsEngine-config-cos-ambari)
