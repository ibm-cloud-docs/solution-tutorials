---
copyright:
  years: 2018, 2019
lastupdated: "2019-03-08"

---

{:java: #java .ph data-hd-programlang='java'}
{:swift: #swift .ph data-hd-programlang='swift'}
{:ios: #ios data-hd-operatingsystem="ios"}
{:android: #android data-hd-operatingsystem="android"}
{:shortdesc: .shortdesc}
{:new_window: target="_blank"}
{:codeblock: .codeblock}
{:screen: .screen}
{:tip: .tip}
{:pre: .pre}
{:important: .important}

# Streaming Analytics using Apache Kafka with Hive
{: #streaming-analytics-iae}

In this tutorial you will build a streaming analytics pipeline which collects and stores Kafka streaming data into {{site.data.keyword.cos_short}}. Simple operations are done on the streaming data before pushing it to {{site.data.keyword.cos_short}}. 

{:shortdesc}

## Objectives
{: #objectives}

* Understand Apache Kafka publish-subscribe messaging.
* Perform cleaning and transformation operations on the streaming data.
* Push the cleaned data to a persistent {{site.data.keyword.cos_short}}.
* Analyse the data with the help of Hive queries.

## Services used
{: #services}

<!-- Please Note when creating links:  

For anchors within the same document always only use the following format:
  [link_description](#anchor_name) 

For anchors or any links to external documents, even for those are are within our tutorials use the following format: 
  [following these steps](https://{DomainName}/docs/cli?topic=cloud-cli-ibmcloud-cli#overview)

If you have an old format html link that you are trying to translate to the new ?topic= format there are two methods you can use:
  1. Try entering the link uri, i.e. /docs/tutorials/serverless-api-webapp.html in the test.cloud.ibm.com, i.e. https://test.cloud.ibm.com/docs/tutorials/serverless-api-webapp.html, you will be redirected to the new ?topic= format which is: https://test.cloud.ibm.com/docs/tutorials?topic=solution-tutorials-serverless-api-webapp#serverless-api-webapp

  2. If the redirect does not work, you may be able to use the old https://console.bluemix.net and enter the uri for the document you are searching for, some times the document uri changed, but you cn recognize it from reading the content and comparing back to the https://test.cloud.ibm.com and obtain the new link

Finally refer to the link topic under the content and design documentation if you have any other questions: https://test.cloud.ibm.com/docs/developing/writing?topic=writing-linking#linking

-->

This tutorial uses the following runtimes and services:
* [IBM Analytics Engine](https://{DomainName}/catalog/services/analytics-engine)
* [Event Streams](https://{DomainName}/catalog/services/message-hub)
* [Cloud Object Storage](https://{DomainName}/catalog/services/cloud-object-storage)

This tutorial may incur costs. Use the [Pricing Calculator](https://{DomainName}/pricing/) to generate a cost estimate based on your projected usage.

## Architecture
{: #architecture}

<p style="text-align: center;">

  ![Architecture](images/solution48-streaming-analytics-iae/Architecture.png)
</p>

1. Application generates log events to {{site.data.keyword.messagehub}}
2. Log event is intercepted and analyzed by Spark Streaming in {{site.data.keyword.iae_short}}
3. Log event is appended to a CSV file located in {{site.data.keyword.cos_short}}
4. Auditor or support staff issues SQL job
5. Hive executes on log file in {{site.data.keyword.cos_short}}
6. Result set is stored in {{site.data.keyword.cos_short}} and delivered to auditor and support staff


## Before you begin
{: #prereqs}

1. Install all the necessary command line (CLI) tools by [following these steps](https://{DomainName}/docs/cli?topic=cloud-cli-ibmcloud-cli#overview).

## Service and Environment Setup
{: #setup}

In this section, you will create the services required to perform analysis of log events generated by your applications. 

1. From the [{{site.data.keyword.Bluemix_short}} catalog](https://{DomainName}/catalog) navigate to the **Integration** section. Create an instance of [Event Streams](https://{DomainName}/catalog/services/event-streams).  Give the service a name and choose any region to deploy in. Name your service `log-analysis-hub`.
2. From the [{{site.data.keyword.Bluemix_short}} catalog](https://{DomainName}/catalog) navigate to the **Storage** section. Create an instance of [Cloud Object Storage](https://{DomainName}/catalog/services/cloud-object-storage). Name your object store `log-analysis-cos`. In the resource page navigate to the **Service Credentials** tab and click on the **New Credential** button. Select the **Include HMAC Credential** option, give an appropriate name to the credential and click **Add**.

## Create Analytics Engine instance
{: #ae_setup}

1. From the [{{site.data.keyword.Bluemix_short}} catalog](https://{DomainName}/catalog) navigate to the **Analytics** section. Create an instance of [Analytics Engine](https://{DomainName}/catalog/services/analytics-engine).  Give the service a name and choose any region to deploy in. Click the **Configure** button.
2. On the next screen, select `Default` for the Hardware configuration and enter the number of compute nodes you will need. 
3. Select the **AE 1.1 Spark and Hadoop** as the Software Package 
4. Under `Advanced Options` enter the below given configuration options for the Object Storage which was created in the previous step. 
```
{
  "core-site": {
    "fs.cos.<servicename1>.access.key": "<userKey>",
    "fs.cos.<servicename1>.endpoint": "<cosEndpoint>",
     "fs.cos.<servicename1>.secret.key": "<SecretKey>"
            }
}
```
The access key, secret key can be obtained from the **Service Credentials** tab of your {{site.data.keyword.cos_short}} resource. The `Endpoint` can be obtained from the **Endpoint** tab of the same page. Make sure to use the private endpoint. 

Click the **Create** button to create the {{site.data.keyword.iae_short}} instance. After the cluster has been provisioned, navigate to the **Service Credentials** tab of the resource and click on the **New Credential** button. Give an appropriate name for the credential and select the **Role** as `Writer`.

## Create a messaging topic and {{site.data.keyword.cos_short}} Bucket
{: #section_one}

Begin by creating an {{site.data.keyword.messagehub}} topic and {{site.data.keyword.cos_short}} bucket. Topics define where applications deliver messages in publish-subscribe messaging systems. After messages are received and processed, they will be stored within a file located in an {{site.data.keyword.cos_short}} bucket.

1.	In your browser, access the `log-analysis-hub` service instance from the Resources.
2.	Click the **+** button to create a topic.
3.	Enter the **Topic Name** `webserver` and click the **Create topic** button.
4.	Click **Service Credentials** and the **New Credential** button.
5.	In the resulting dialog, type `webserver-flow` as the **Name** and click the **Add** button.
6.	Click **View Credentials** and copy the information to a safe place. It will be used in the next section.
7.	Back in the [Resource List](https://{DomainName}/resources?search=log-analysis), select the `log-analysis-cos` service instance.
8.	Click **Create bucket**.
    * Enter a unique **Name** for the bucket.
    * Select **Cross Region** for **Resiliency**.
    * Select **us-geo** as the **Location**.
    * Click **Create bucket**.


## Using Kafka console tools with Event Streams
{: #kafkatools}
1.	Download and unzip the [Kafka client](apache.claz.org/kafka/2.2.0/kafka-2.2.0-src.tgz)
2.	Change directory to `bin` and create a text file named ``message-hub.config`` with the following contents.

```sh
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule 

required username="USER" password="PASSWORD";
 security.protocol=SASL_SSL
 sasl.mechanism=PLAIN
 ssl.protocol=TLSv1.2
 ssl.enabled.protocols=TLSv1.2
 ssl.endpoint.identification.algorithm=HTTPS

```
 {: pre}

3. Replace `USER` and `PASSWORD` in your `message-hub.config` file with the user and password values seen in **Service Credentials** from the previous section. Save `message-hub.config`.
4. From the `bin` directory, run the following command. Replace `KAFKA_BROKERS_SASL` with the `kafka_brokers_sasl` value seen in **Service Credentials**. 
5. The Kafka console tool is awaiting input. Sample data provided in the repository ( Hyperlink to the file in the repository ) can be pasted into the console or the entire file can be piped to the prompt of the producer script. An example is given below: 
If the data is stored in a file: 
 {: pre}
```sh
./kafka-console-producer.sh --broker-list KAFKA_BROKERS_SASL \
 --producer.config message-hub.config --topic webserver
 ./kafka-console-producer.sh --broker-list \
 kafka04-prod02.messagehub.services.us-south.bluemix.net:9093,\
 kafka05-prod02.messagehub.services.us-south.bluemix.net:9093,\
 kafka02-prod02.messagehub.services.us-south.bluemix.net:9093,\
 kafka01-prod02.messagehub.services.us-south.bluemix.net:9093,\
 kafka03-prod02.messagehub.services.us-south.bluemix.net:9093 \
 --producer.config message-hub.config --topic webserver > data.json
```
If the data is to be pasted paste the following after running the kafka-console-producer script.  
 {: pre}
```
{"InvoiceNo":5370812,"StockCode":"85114B","Description":"IVORY ENCHANTED FOREST PLACEMAT","Quantity":24,"InvoiceDate":1291593600000,"UnitPrice":1.65,"CustomerID":15332,"Country":"Lithuania}
{"InvoiceNo":5382402,"StockCode":22816,"Description":"CARD MOTORBIKE SANTA","Quantity":24,"InvoiceDate":1292025600000,"UnitPrice":0.42,"CustomerID":16062,"Country":"United Kingdom}
{"InvoiceNo":5382402,"StockCode":22086,"Description":"PAPER CHAIN KIT 50'S CHRISTMAS ","Quantity":40,"InvoiceDate":1292025600000,"UnitPrice":2.55,"CustomerID":16062,"Country":"United Kingdom}
{"InvoiceNo":5382402,"StockCode":21733,"Description":"RED HANGING HEART T-LIGHT HOLDER","Quantity":6,"InvoiceDate":12920200000,"UnitPrice":2.95,"CustomerID":16062,"Country":"United Kingdom"
```


## Running Spark Streaming Job on {{site.data.keyword.iae_short}} 
{: #spark_ae}

In this section you will run a Spark Streaming job on {{site.data.keyword.iae_short}} which will take a stream of data from Kakfa, perform some transformations, clean the data and then push the modified data to a {{site.data.keyword.cos_short}} bucket. 

In the repository, sample data of Online Retail transactions is given. The data is cleaned by the code to remove all fields where `InvoiceNo` or `CustomerId` is null and if the `Quantity` is less than 0. An extra column is also added to the data which is the product of two rows ( `Quantity` and `UnitPrice`). The fields of a particular country (`Lithuania` ) are then filtered out and pushed to a {{site.data.keyword.cos_short}} bucket. 

Clone the repository (Hyperlink to the file in the repository ) containing the sample code for submitting and running Spark Streaming on {{site.data.keyword.iae_short}}. Run the following commands to package the Scala program and copy it to  	{{site.data.keyword.iae_short}}. 

1.	git clone (name of repository) 
2.	cd (name of the folder) 
3.	sbt package 
4.	scp target/scala-2.11/spark-structured-streaming-on-iae-to-cos_2.11-1.0.jar clsadmin@chs-xxxxx-mn003.(changeme).ae.appdomain.cloud:./
Cluster credentials can be obtained from the Service Credentials tab of the Analytics Engine service. The endpoint can be found under ambari_console. 
5.	Create a jaas.conf for spark to authenticate to  	{{site.data.keyword.messagehub}}. 
```sh
cat << EOF > jaas.conf
KafkaClient {
    org.apache.kafka.common.security.plain.PlainLoginModule required
    serviceName="kafka"
    username="CHANGEME"
    password="CHANGEME";
};
EOF
```

Replace `CHANGEME` in the file with the username and password of the `Event Stream` instance. This can be obtained from the **Service Credentials** tab of the `Event Streams` service. 

6.	Create a script which contains configuration variables needed by Spark. 

```sh
cat << EOF > config_vars.sh

# S3 #
S3_ACCESSKEY=CHANGEME
S3_SECRETKEY=CHANGEME
S3_PRIVATE_ENDPOINT=CHANGEME # E.g. s3.eu-geo.objectstorage.service.networklayer.com
S3_BUCKET=CHANGEME

# KAFKA #
KAFKA_BOOTSTRAP_SERVERS=HOST1:PORT1,HOST2:PORT2,... # E.g. 
    kafka01-prod02.messagehub.services.us-south.bluemix.net:9093,
    kafka02-prod02.messagehub.services.us-south.bluemix.net:9093,
    kafka05-prod02.messagehub.services.us-south.bluemix.net:9093,
    kafka03-prod02.messagehub.services.us-south.bluemix.net:9093,
    kafka04-prod02.messagehub.services.us-south.bluemix.net:9093

EOF
```
S3 access key and secret key can be obtained from the service credential created before. The value of the private endpoint can be obtained from the **Endpoints** tab of the {{site.data.keyword.cos_short}} service. 
Kafka bootstrap server can be obtained from the **Service Credentials** tab of the {{site.data.keyword.messagehub}} service. 

7.	Create a script to start spark interactively via yarn
```sh
cat << 'EOF' > start_yarn_client.sh
source config_vars.sh

spark-submit --class main.Main \
       --master yarn \
       --deploy-mode client \
       --files jaas.conf \
       --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0 \
       --conf "spark.driver.extraJavaOptions=-Djava.security.auth.login.config=jaas.conf" \
       --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=jaas.conf" \
       --conf spark.s3_accesskey=$S3_ACCESSKEY \
       --conf spark.s3_secretkey=$S3_SECRETKEY \
       --conf spark.s3_endpoint=$S3_PRIVATE_ENDPOINT \
       --conf spark.s3_bucket=$S3_BUCKET \
       --conf spark.kafka_bootstrap_servers=$KAFKA_BOOTSTRAP_SERVERS \
       --conf spark.trigger_time_ms=30000 \
       --num-executors 1 \
       --executor-cores 1 \
       spark-structured-streaming-on-iae-to-cos_2.11-1.0.jar
EOF
chmod +x start_yarn_client.sh
```
Run the interactive script, you should see spark saving to {{site.data.keyword.cos_short}} and output sent to the terminal:
```sh
bash -x start_yarn_client.sh
```
Navigate to your bucket in the {{site.data.keyword.cos_short}} resource page to see the csv files. 


## Analysing the data using Hive 
{: #hive_query}

The data pushed to cos can be queried using Hive through {{site.data.keyword.iae_short}}. 

1.	Connect to the Hive server by using with `Beeline` client.
```sh
ssh clsadmin@chs-xxxxx-mn003.<changeme>.ae.appdomain.cloud 
beeline -u ‘jdbc:hive2://chs-xxxxx-mn001.<change-me>.ae.appdomain.cloud:8443/;ssl=true;transportMode=http;httpPath=gateway/default/hive’ -n clsadmin -p <password> 
```
The hive_jdbc service endpoint can be found under the **service credential** tab of the  	{{site.data.keyword.iae_short}} resource page. 

Enter the username and password of the  	{{site.data.keyword.iae_short}} cluster when prompted for. 

2.	An external hive table can be created with the following command. 
```sql
CREATE EXTERNAL TABLE myhivetable(InvoiceNo INT, StockCode STRING, Description STRING, Quantity INT,  InvoiceDate STRING, UnitPrice DOUBLE, CustomerID INT, Country STRING, Total Price FLOAT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 'cos://<bucket-name>.<service-name>/<directory-name>/';
```
3.	The data can be viewed with the following command. 
```sql
SELECT * FROM myhivetable; 
```

## Related content
{: #related}

* [Working with Hive](https://{DomainName}/docs/services/AnalyticsEngine?topic=AnalyticsEngine-working-with-hive)

