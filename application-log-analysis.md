---
copyright:
  years: 2017, 2019
lastupdated: "2019-04-08"


---

{:shortdesc: .shortdesc}
{:new_window: target="_blank"}
{:codeblock: .codeblock}
{:screen: .screen}
{:tip: .tip}
{:pre: .pre}

# Analyze logs and monitor health of a Kubernetes application
{: #application-log-analysis}

> Work in progress

This tutorial shows how the [{{site.data.keyword.la_full_notm}}](https://{DomainName}/observe/logging) service can be used to understand and diagnose activities of a Kubernetes app that is deployed on {{site.data.keyword.Bluemix_notm}}. You will deploy a Python Kubernetes application, configure a LogDNA agent, generate different types of logs. Then, you will search, filter and analyze those logs through {{site.data.keyword.la_short}} Web UI. Moreover, you will also setup the [{{site.data.keyword.mon_full_notm}}](https://{DomainName}/observe/monitoring) service to monitor the performance and health of your application.

## Objectives
* Provision the {{site.data.keyword.la_short}} service
* Deploy a Python Kubernetes application
* Generate different types of log entries
* Access application logs
* Search and filter logs
* Setup {{site.data.keyword.mon_short}}

## Services used
{: #services}

This tutorial uses the following runtimes and services:  
* [{{site.data.keyword.la_full_notm}}](https://{DomainName}/observe/logging)  
* [{{site.data.keyword.mon_full_notm}}](https://{DomainName}/observe/monitoring)

This tutorial may incur costs. Use the [Pricing Calculator](https://{DomainName}/pricing/) to generate a cost estimate based on your projected usage.

## Architecture
{: #architecture}

{{site.data.keyword.Bluemix_notm}} offers three complementary services that help to obtain insights into application health, stability and usage:  
* The [{{site.data.keyword.la_short}}](https://{DomainName}/observe/logging) service provides an easy-to-use interface to logs generated by applications running in the {{site.data.keyword.Bluemix_notm}}. In the premium plans, external log events can also be fed into the service for consolidated storage and analysis.  
* The [{{site.data.keyword.mon_short}}](https://{DomainName}/observe/monitoring) service can be used to perform regular tests on an application to check availability, including speed.  
* Last, the [{{site.data.keyword.cloudaccesstraillong_notm}}](https://{DomainName}/catalog/services/activity-tracker) has the capability to capture, store and visualize activities performed by {{site.data.keyword.Bluemix_notm}} users and services in your account. Captured events can be stored and analyzed, e.g., to investigate security breaches or unauthorized access.  

<p style="text-align: center;">

  ![](images/solution12/Architecture.png)
</p>


## Prerequisites

{: #prereq}

* [Install {{site.data.keyword.dev_cli_notm}}](https://{DomainName}/docs/cli?topic=cloud-cli-ibmcloud-cli#ibmcloud-cli) - Script to install docker, kubectl, helm, ibmcloud cli and required plug-ins.
* [Set up the {{site.data.keyword.registrylong_notm}} CLI and your registry namespace](https://{DomainName}/docs/services/Registry?topic=registry-registry_setup_cli_namespace#registry_setup_cli_namespace).
* [Check whether your User ID is assigned the right platform role to perform required actions](https://{DomainName}/docs/services/Log-Analysis-with-LogDNA?topic=LogDNA-iam#platform)

## Create a Kubernetes cluster
{: #create_cluster}

{{site.data.keyword.containershort_notm}} provides an environment to deploy highly available apps in Docker containers that run in Kubernetes clusters.

Skip this section if you have an existing **Standard** cluster and want to reuse with this tutorial.
{: tip}

1. Create **a new Cluster** from the [{{site.data.keyword.Bluemix}} catalog](https://{DomainName}/kubernetes/catalog/cluster/create) and choose the **Standard** cluster.

	Log forwarding is *not* enabled for the **Free** cluster.
	{:tip}

1. Select a resource group and Geography.
1. For convenience, use the name `mycluster` to be consistent with this tutorial.
1. Select a **Worker Zone** and select the smallest **Machine type** with 2 **CPUs** and 4 **GB RAM** as it is sufficient for this tutorial. 
1.. Select 1 **Worker node** and leave all other options set to defaults. Click **Create Cluster**.
1. Check the status of your **Cluster** and **Worker Node** and wait for them to be **ready**.

## Provision a {{site.data.keyword.la_short}} instance
From the movement, an application is deployed to a {{site.data.keyword.containerlong_notm}} cluster in {{site.data.keyword.Bluemix_notm}}, the app starts generating diagnostic output, i.e. logs and you want to access logs to troubleshoot problems and pre-empt issues. At any time, you want to have access to different types of logs such as worker logs, pod logs, app logs, or network logs. By using the {{site.data.keyword.la_short}} service, it is possible to aggregate logs from various sources and retain them as long as needed. This allows to analyze the "big picture" when required and to troubleshoot more complex situations.

To provision a {{site.data.keyword.la_short}} service, 

1. Navigate to [observability](https://{DomainName}/observe/logging) page and under **logging**, click **Create instance**.
2. Provide a unique **Service name**.
3. Choose a region/location and Select a resource group.
4. Select **7 day Log Search** as your plan and Click **Create**. 

With this you configured a centralized log management system where log data is hosted on IBM Cloud.


## Deploy and configure a Kubernetes app to forward logs
The ready-to-run [code for the logging app is located in this Github repository](https://github.com/IBM-Cloud/application-log-analysis). The application is written using [Django](https://www.djangoproject.com/), a popular Python server-side web framework. Clone or download the repository, then deploy the app to {{site.data.keyword.containershort_notm}} on {{site.data.keyword.Bluemix_notm}}.

### Deploy the python application
On a terminal,

1. Clone the Github repository:

   ```sh
   git clone https://github.com/IBM-Cloud/application-log-analysis
   cd application-log-analysis
   ```
   {: pre}
1. [Build the Docker image](https://{DomainName}/docs/services/Registry?topic=registry-registry_images_#registry_images_creating) in {{site.data.keyword.registryshort_notm}}.
   - Find the **Container Registry** with `ibmcloud cr info`, such as us.icr.io or uk.icr.io.
   - Create a namespace to store the container image.
   
      ```sh
      ibmcloud cr namespace-add app-log-analysis-namespace
      ```
      {: pre}
      
   - Replace `<CONTAINER_REGISTRY>` with your container registry value and use **app-log-analysis** as the image name.

	   ```sh
	   ibmcloud cr build -t <CONTAINER_REGISTRY>/app-log-analysis-namespace/app-log-analysis:latest .
	   ```
	   {: pre}
	   
   - Replace the **image** value in `app-log-analysis.yaml` file with the image tag `<CONTAINER_REGISTRY>.icr.io/app-log-analysis-namespace/app-log-analysis:latest`
   
1. Retrieve the cluster configuration and set the `KUBECONFIG` environment variable.

   ```sh
   $(ibmcloud ks cluster-config --export mycluster)
   ```
   {: pre}
1. Deploy the app.

   ```sh
   kubectl apply -f app-log-analysis.yaml
   ```
   {: pre}
   
1. To access the application, you need `public IP` of the worker node and the `NodePort`
	1. For public IP, run the following command
	
		```sh
		ibmcloud ks workers mycluster
		```
		{: pre}
	1. For the NodePort which will be 5-digits (e.g., 3xxxx), run the below command
	
		```sh
		kubectl describe service app-log-analysis-svc
		```
		{: pre}
		
		You can now access the application at `http://worker-ip-address:portnumber`
		
### Configure the cluster to send logs to your LogDNA instance

To configure your Kubernetes cluster to send logs to your {{site.data.keyword.la_full_notm}} instance, you must install a logdna-agent pod on each node of your cluster. The LogDNA agent reads log files from the pod where it is installed, and forwards the log data to your LogDNA instance.

1. Navigate to [observability](https://{DomainName}/observe/logging) page and click **Logging**.
2. Click on **Edit log resources** next to the service which you created earlier and select **Kubernetes**.
3. Copy and run the first command on a terminal where you have set the KUBECONFIG to create a kubernetes secret with the logDNA ingestion key for your service instance.
4. Copy and run the second command to deploy a logDNA agent on every worker node of your Kubernetes cluster. The LogDNA agent collects logs with the extension *.log and extensionsless files that are stored in the /var/log directory of your pod. By default, logs are collected from all namespaces, including kube-system, and automatically forwarded to the {{site.data.keyword.la_full_notm}} service.
5. After you configure a log source, launch the logDNA UI by clicking **View LogDNA**. It may take a few minutes before you start seeing logs.

Let's generate some application logs and view them in logDNA UI.
   
## Generate application logs
Next, in order to work with application logs, you first need to generate some. The deploy process above already generated many log entries. 

1. Visit the web app at `http://worker-ip-address:portnumber`.
2. The application allows you to log a message at a chosen log level. The available log levels are **critical**, **error**, **warn**, **info** and **debug**. The application's logging infrastructure is configured to allow only log entries on or above a set level to pass. Initially, the logger level is set to **warn**. Thus, a message logged at **info** with a server setting of **warn** would not show up in the diagnostic output. The UI allows to change the logger setting for the server log level as well. Try it and generate several log entries.
3. Take a look at the code in the file [**views.py**](https://github.com/IBM-Cloud/application-log-analysis/blob/master/app/views.py). The code contains **print** statements as well as calls to **logger** functions. Printed messages are written to the **stdout** stream (regular output, application console / terminal), logger messages appear in the **stderr** stream (error log).
4. Back in the application, generate several log entries by submitting messages at different levels. Change the server-side log level in-between to make it more interesting.

### Access application logs
You can access the application specific log in the logDNA UI using the filters.

1. On the top bar, click on **All Apps**.
2. Under containers, check **app-log-analysis**. A new unsaved view is shown with application logs of all levels. 
3. To see logs of specific log level, Click on **All Levels** and select multiple levels like Error,info, warning etc.,

## Search and analyze logs
The {{site.data.keyword.la_short}} / Kibana dashboard, by default shows all available log entries from the past 15 minutes. Most recent entries are shown on the top and automatic refresh is turned off by default. The visible bar chart represents the count of messages per 30 seconds over those 15 minutes. In this section, you will modify what and how much is displayed and save this as **search query** for future use.

1. Available fields that can be displayed and queried are shown on the left. Locate and click on **message > add**.  
![](images/solution12/Dashboard_MessagesAdded.png)
2. If you are seeing logs for more than one application, filter them based on the **app_name_str** field. Instead of **add** use the **+** next to an app name to only see entries for that application or the **-** to exclude the app's logs from the list.   
![](images/solution12/app_name_str.png)
3. Adjust the displayed interval by navigating to the upper right and clicking on **Last 15 minutes**. Adjust the value to **Last 24 hours**.
4. Next to the configuration of the interval is the auto-refresh setting. By default it is switched off, but you can change it.  
5. Below the configuration is the search field. Here you can [enter and define search queries](https://{DomainName}/docs/services/CloudLogAnalysis/kibana?topic=cloudloganalysis-define_search#define_search). To filter for all logs reported as app errors and containing one of the defined log levels, enter the following:   
```
message:(CRITICAL|INFO|ERROR|WARNING|DEBUG) && message_type_str:ERR
```   
It should look like shown below. The displayed log entries are now filtered based on the search criteria.   
![](images/solution12/SearchForMessagesERR.png)   
6. Store the search criteria for future use by clicking **Save** in the configuration bar. Use **ERRlogs** as name.


## Visualize logs
Now that you have a query defined, in this section you will use it as foundation for a chart, a visualization of that data. You will first create visualizations and then use them to compose a dashboard.

### Pie chart as donut
1. Click on **Visualize** in the left navigation bar.
2. In the list of offered visualizations Locate **Pie chart** and click on it.
3. Select the query **ERRlogs** that you saved earlier.
4. On the next screen, under **Select buckets type**, select **Split Slices**, then for **Aggregation** choose **Filters**. Add 5 filters having the values of **CRITICAL**, **ERROR**, **WARNING**, **INFO** and **DEBUG** as shown here:   
![](images/solution12/VisualizationFilters.png)   
6. Click on **Options** (right to **Data**) and activate **Donut** as view option. Finally, click on the **play** icon to apply all changes to the chart. Now you should see a **Donut Pie Chart** similar to this one:   
![](images/solution12/Donut.png)   
7. Adjust the displayed interval by navigating to the upper right and clicking on **Last 15 minutes**. Adjust the value to **Last 24 hours**.
8. Save the visualization as **DonutERR**.

### Metric
Next, create another visualization for **Metric**.
1. Pick **Metric** from the list of offered visualizations. In step 2, on the left side, click on the name beginning with **[logstash-]**.
2. On the next screen, expand **Metric** to be able to enter a custom label. Add **Log Entries within 24 hours** and click on the **play** icon to update the shown metric.   
![](images/solution12/Metric_LogCount24.png)   
3. Save the visualization as **LogCount24**.

### Dashboard
Once you have added visualizations, they can be used to compose a dashboard. A dashboard is used to display all important metrics and to help indicate the health of your apps and services.
1. Click on **Dashboard** in the left navigation panel, then on **Add** to start placing existing visualizations onto the empty dashboard.
2. Add the log count on the left and the donut chart on the right. Change the size of each component and to move them as desired.
3. Click on the arrow in the lower left corner of a component to view changes to a table layout and additional information about the underlying request, response and execution statistics are offered.
![](images/solution12/DashboardTable.png)   
4. Save the dashboard for future use.

## Add {{site.data.keyword.mon_short}}
In the following, you are going to add {{site.data.keyword.mon_short}} to the application. The service regularly checks the availability and response time of the app. It can be configured to raise alerts of different severity when thresholds are passed.

You can also run synthetic tests to measure performance of webpage loads, API calls and simulated user flows through scripted browser interaction using Selenium. See [**Creating a REST API test**](https://{DomainName}/docs/services/AvailabilityMonitoring?topic=availability-monitoring-avmon_rest_api#avmon_rest_api) and how to **create a test script** from either an [uploaded file](https://{DomainName}/docs/services/AvailabilityMonitoring?topic=availability-monitoring-avmon_upload_script_test#avmon_upload_script_test) or from a [GitHub repository](https://{DomainName}/docs/services/AvailabilityMonitoring?topic=availability-monitoring-avmon_git_script_test#avmon_git_script_test) in the [{{site.data.keyword.mon_short}} docs](https://{DomainName}/docs/services/AvailabilityMonitoring?topic=availability-monitoring-avmon_gettingstarted#avmon_gettingstarted).
{:tip}   

1. Add the {{site.data.keyword.mon_short}} by accessing the dashboard in the [{{site.data.keyword.Bluemix_notm}} console](https://{DomainName}) and then either provision it as a new service in your organization and space, or by clicking on the application to open the details page and then clicking on **Monitoring**. Use the approach via the application details. The following shows that no tests have been run, but one default test is present.   
![](images/solution12/AvailabilityMonitoringAdded.png)   
2. Click on **View All Tests** to switch to the monitoring dashboard.
3. On the tile with the default test click on the menu in the upper right to **Edit** the test.
4. In the settings change the interval from 15 minutes to 1 hour. Add another test location by clicking on a city name. Also **Add Condition** to check the header response code for anything greater or equal to 400. Click **Finish** to be taken back to the monitoring dashboard.
5. Click the **configure** icon on the upper right. In the pop out you can access the account usage, enable alter notification and change the dashboard refresh rate. Alert notifications allow to set up notification rules and integrations with various communication tools.

## Expand the tutorial
Do you want to learn more? Here are some ideas of what you can do next:
* Push the same app again with a different name or use the [app deployed in a Kubernetes cluster](https://{DomainName}/docs/services/CloudLogAnalysis?topic=cloudloganalysis-analyzing_logs_Kibana#analyzing_logs_Kibana). Then, the {{site.data.keyword.la_short}} dashboard (Kibana) will show the combined logs of all apps.
* Filter by a single app.
* Add a saved search and metric only for critical and error events.
* Build a dashboard for all your apps.
* Add more availability tests.
* [Configure alert notifications](https://{DomainName}/docs/services/AvailabilityMonitoring?topic=availability-monitoring-avmon_notifications#avmon_notifications).


## Related content
{:related}

* [Documentation for {{site.data.keyword.loganalysislong_notm}}](https://{DomainName}/docs/services/CloudLogAnalysis?topic=cloudloganalysis-getting-started-with-cla#getting-started-with-cla)
* [Documentation for {{site.data.keyword.mon_short}}](https://{DomainName}/docs/services/AvailabilityMonitoring?topic=availability-monitoring-avmon_gettingstarted#avmon_gettingstarted)
* [Logging facility for Python](https://docs.python.org/3/library/logging.html)
* [IBM Cloud Log Collection API](https://{DomainName}/apidocs/log-analysis-api)
* Kibana User Guide: [Discovering Your Data](https://www.elastic.co/guide/en/kibana/5.1/tutorial-discovering.html)
* Kibana User Guide: [Visualizing Your Data](https://www.elastic.co/guide/en/kibana/5.1/tutorial-visualizing.html)
* Kibana User Guide: [Putting it all Together with Dashboards](https://www.elastic.co/guide/en/kibana/5.1/tutorial-dashboard.html)
